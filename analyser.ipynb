{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up and simplifying the dateframe to reduce memory and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, do: pip install pandas fastparquet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "URL_LOOKUP = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_item.parquet'\n",
    "URL_PREMISE = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_premise.parquet'\n",
    "url_file = 'pricecatcher/pricecatcher/price_urls.json'\n",
    "\n",
    "malaysia_states = {\n",
    "    \"Johor\" : 1,\n",
    "    \"Kedah\" : 2,\n",
    "    \"Kelantan\" : 3,\n",
    "    \"Melaka\": 4,\n",
    "    \"Negeri Sembilan\" : 5,\n",
    "    \"Pahang\" : 6,\n",
    "    \"Perak\" : 7,\n",
    "    \"Perlis\" : 8,\n",
    "    \"Pulau Pinang\" : 9,\n",
    "    \"Sabah\" : 10,\n",
    "    \"Sarawak\" : 11,\n",
    "    \"Selangor\" : 12,\n",
    "    \"Terengganu\" :14,\n",
    "    \"Wilayah Persekutuan\" : 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dfs = []\n",
    "\n",
    "with open(url_file, 'r') as json_file: \n",
    "    price_urls_data = json.load(json_file)\n",
    "\n",
    "for entry in price_urls_data:\n",
    "        parquet_urls = entry['parquet_files']\n",
    "        for url in parquet_urls:\n",
    "            df = ddf.read_parquet(url, blocksize = '1GB', npartitions = 8)\n",
    "            if 'date' in df.columns: df['date'] = ddf.to_datetime(df['date'])\n",
    "            df = df[(df['item_code'] != -1) | (df['premise_code'] != -1)]\n",
    "            price_dfs.append(df)\n",
    "\n",
    "price = ddf.concat(price_dfs, ignore_index = True)\n",
    "\n",
    "premise = ddf.read_parquet(URL_PREMISE, npartitions = 8)\n",
    "lookup = ddf.read_parquet(URL_LOOKUP, npartitions = 8)\n",
    "premise = premise.dropna()\n",
    "lookup = lookup.dropna()\n",
    "if 'date' in lookup.columns: lookup['date'] = ddf.to_datetime(lookup['date'])\n",
    "if 'date' in premise.columns: premise['date'] = ddf.to_datetime(premise['date'])\n",
    "\n",
    "#premise = premise.drop(columns = ['premise_type', 'address'])\n",
    "price['premise_code'] = price['premise_code'].astype('int32')\n",
    "premise['premise_code'] = premise['premise_code'].astype('int32')\n",
    "price = premise.merge(price, on='premise_code',how='left',indicator=False)\n",
    "price = price.drop(columns = ['premise_type', 'address', 'premise', 'premise_code', 'district'])\n",
    "\n",
    "del price_dfs\n",
    "del df\n",
    "del entry\n",
    "del json_file\n",
    "del parquet_urls\n",
    "del url\n",
    "del URL_PREMISE\n",
    "del URL_LOOKUP\n",
    "del url_file\n",
    "del price_urls_data\n",
    "del premise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A DICT SO USER CAN ACCESS USING ITEM AND PREMISE CODE INSTEAD OF NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652ffb41891245c8a6ab08bd3be21512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lookup_dict = {}\n",
    "\n",
    "\n",
    "for index, row in tqdm(lookup.iterrows(), total=len(lookup)):\n",
    "    key_tuple = tuple([row['item'], row['item_category']])\n",
    "    lookup_dict[key_tuple] = row['item_code']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lookup_dict.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(price.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DDF based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from fuzzywuzzy import fuzz\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining global variable below\n",
    "user_input = []\n",
    "filtered_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(input, choices, threshold=80):\n",
    "        match_score = [(choice, fuzz.partial_ratio(input, choice.lower())) for choice in choices if isinstance(choice, str)]\n",
    "        matched_item = max(match_score, key=lambda x: x[1], default=None)\n",
    "        if matched_item[1] >= threshold:\n",
    "            return matched_item[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def identify_item_code(user_input):\n",
    "    item_keys = []\n",
    "    user_item = user_input.lower()\n",
    "    matching_item_codes = []\n",
    "    \n",
    "    for key in lookup_dict.keys(): \n",
    "    # Search in the \"item\" \n",
    "        if match(user_item, [key[0]], threshold=80) is not None:\n",
    "            item_codes = lookup_dict.get(key, [])\n",
    "            matching_item_codes.append(item_codes)\n",
    "    \n",
    "    # Search in the \"item_category\" \n",
    "        if match(user_item, [key[1]], threshold=80) is not None:\n",
    "            item_codes = lookup_dict.get(key, [])\n",
    "            matching_item_codes.append(item_codes)\n",
    "    \n",
    "    item_keys = list(set(matching_item_codes))\n",
    "    return price[price['item_code'].isin(item_keys)]\n",
    "\n",
    "def identify_state(user_input, df):\n",
    "    state_name = user_input.lower()\n",
    "    matching_item_codes=[]\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        matched_state = match(state_name, df['state'], threshold=80)\n",
    "        return chunk[chunk['state'].isin(matched_state)]\n",
    "    \n",
    "    chunk_size = 10000  # Adjust this based on your system's capabilities\n",
    "    chunks = [df[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(process_chunk, chunks))\n",
    "    \n",
    "    return ddf.concat(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just search using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = 'ayam'\n",
    "state = 'johor'\n",
    "\n",
    "if (len(item.strip()) > 0) and (len(state.strip()) > 0): user_input = [item, state]\n",
    "elif (len(item.strip()) > 0): user_input = [item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(user_input) >= 1: \n",
    "    filtered_data = identify_item_code(user_input[0])\n",
    "\n",
    "if len(user_input) >= 2: \n",
    "    filtered_data = identify_state(user_input[1], filtered_data)\n",
    "print(filtered_data.compute())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
