{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up and simplifying the dateframe to reduce memory and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, do: pip install pandas fastparquet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_LOOKUP = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_item.parquet'\n",
    "URL_PREMISE = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_premise.parquet'\n",
    "url_file = 'pricecatcher/pricecatcher/price_urls.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_dfs = []\n",
    "\n",
    "with open(url_file, 'r') as json_file: \n",
    "    price_urls_data = json.load(json_file)\n",
    "\n",
    "for entry in price_urls_data:\n",
    "        parquet_urls = entry['parquet_files']\n",
    "        for url in parquet_urls:\n",
    "            df = ddf.read_parquet(url, blocksize = '1GB', npartitions = 20)\n",
    "            if 'date' in df.columns: df['date'] = ddf.to_datetime(df['date'])\n",
    "            df = df[(df['item_code'] != -1) | (df['premise_code'] != -1)]\n",
    "            price_dfs.append(df)\n",
    "\n",
    "price = ddf.concat(price_dfs, ignore_index = True)\n",
    "\n",
    "premise = ddf.read_parquet(URL_PREMISE, npartitions = 8)\n",
    "lookup = ddf.read_parquet(URL_LOOKUP, npartitions = 8)\n",
    "premise = premise.dropna()\n",
    "lookup = lookup.dropna()\n",
    "if 'date' in lookup.columns: lookup['date'] = ddf.to_datetime(lookup['date'])\n",
    "if 'date' in premise.columns: premise['date'] = ddf.to_datetime(premise['date'])\n",
    "\n",
    "#premise = premise.drop(columns = ['premise_type', 'address'])\n",
    "price['premise_code'] = price['premise_code'].astype('int32')\n",
    "premise['premise_code'] = premise['premise_code'].astype('int32')\n",
    "price = premise.merge(price, on='premise_code',how='left',indicator=False)\n",
    "price = price.drop(columns = ['premise_type', 'address', 'premise', 'premise_code', 'district'])\n",
    "\n",
    "del price_dfs\n",
    "del df\n",
    "del entry\n",
    "del json_file\n",
    "del parquet_urls\n",
    "del url\n",
    "del URL_PREMISE\n",
    "del URL_LOOKUP\n",
    "del url_file\n",
    "del price_urls_data\n",
    "del premise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A DICT SO USER CAN ACCESS USING ITEM AND PREMISE CODE INSTEAD OF NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d49b8e8b89045d9a458bb7e69c65311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lookup_dict = {}\n",
    "\n",
    "for index, row in tqdm(lookup.iterrows(), total=len(lookup)):\n",
    "    key_tuple = tuple([row['item'], row['item_category']])\n",
    "    lookup_dict[key_tuple] = row['item_code']\n",
    "\n",
    "del key_tuple\n",
    "del row\n",
    "del index\n",
    "del lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DDF based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "from distributed import Client, LocalCluster\n",
    "from fuzzywuzzy import fuzz\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(input, choices, threshold=80):\n",
    "        match_score = [(choice, fuzz.partial_ratio(input, choice.lower())) for choice in choices if isinstance(choice, str)]\n",
    "        matched_item = max(match_score, key=lambda x: x[1], default=None)\n",
    "        if matched_item[1] >= threshold:\n",
    "            return matched_item[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def match(input, choices, threshold=80):\n",
    "    match_score = [(choice, fuzz.partial_ratio(input, choice.lower())) for choice in choices if isinstance(choice, str)]\n",
    "    matched_item = max(match_score, key=lambda x: x[1], default=None)\n",
    "    if matched_item[1] >= threshold:\n",
    "        return matched_item[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_chunk(keys_chunk, user_item, lookup_dict):\n",
    "    matching_item_codes = []\n",
    "\n",
    "    for key in keys_chunk:\n",
    "        if match(user_item, [key[0], key[1]], threshold=80) is not None:\n",
    "            item_codes = lookup_dict[key]\n",
    "            matching_item_codes.append(item_codes)\n",
    "\n",
    "    return matching_item_codes\n",
    "\n",
    "def identify_item_code(user_input, lookup_dict, chunk_size=1000):\n",
    "    user_item = user_input.lower()\n",
    "    keys = list(lookup_dict.keys())\n",
    "    keys_chunks = [keys[i:i + chunk_size] for i in range(0, len(keys), chunk_size)]\n",
    "\n",
    "    delayed_partitions = []\n",
    "\n",
    "    for chunk in keys_chunks:\n",
    "        delayed_partitions.append(delayed(process_chunk)(chunk, user_item, lookup_dict))\n",
    "\n",
    "    return delayed_partitions\n",
    "\n",
    "def identify_state(user_input, df):\n",
    "    state_name = user_input.lower()\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        matched_state = match(state_name, df['state'], threshold=80)\n",
    "        return chunk[chunk['state'].isin(matched_state)]\n",
    "    \n",
    "    delayed_partitions = [delayed(process_chunk)(chunk) for chunk in df.to_delayed()]\n",
    "\n",
    "    results = compute(*delayed_partitions, scheduler='threads')\n",
    "    \n",
    "    return ddf.from_delayed(results, meta=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just search using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = []\n",
    "item = 'ayam'\n",
    "state = 'johor'\n",
    "\n",
    "if (len(item.strip()) > 0) and (len(state.strip()) > 0): user_input = [item, state]\n",
    "elif (len(item.strip()) > 0): user_input = [item]\n",
    "del item\n",
    "del state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 110.36 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\base.py:1437: UserWarning: Running on a single-machine scheduler when a distributed client is active might lead to unexpected results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[##############                          ] | 35% Completed | 17.15 ss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ignoring exception in ensure_cleanup_on_exception\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 936, in ensure_cleanup_on_exception\n",
      "    yield\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 951, in shuffle_group_3\n",
      "    p.append(d, fsync=True)\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\encode.py\", line 25, in append\n",
      "    self.partd.append(data, **kwargs)\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\buffer.py\", line 45, in append\n",
      "    self.flush(keys)\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\buffer.py\", line 99, in flush\n",
      "    self.slow.append(dict(zip(keys, self.fast.get(keys))))\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\file.py\", line 42, in append\n",
      "    f.write(v)\n",
      "OSError: [Errno 28] No space left on device\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\dataframe\\shuffle.py\", line 941, in ensure_cleanup_on_exception\n",
      "    p.drop()\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\encode.py\", line 39, in drop\n",
      "    return self.partd.drop()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\buffer.py\", line 78, in drop\n",
      "    self.slow.drop()\n",
      "  File \"c:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\file.py\", line 93, in drop\n",
      "    shutil.rmtree(self.path)\n",
      "  File \"C:\\Users\\camel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py\", line 759, in rmtree\n",
      "    return _rmtree_unsafe(path, onerror)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\camel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py\", line 622, in _rmtree_unsafe\n",
      "    onerror(os.unlink, fullname, sys.exc_info())\n",
      "  File \"C:\\Users\\camel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py\", line 620, in _rmtree_unsafe\n",
      "    os.unlink(fullname)\n",
      "PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\camel\\\\AppData\\\\Local\\\\Temp\\\\tmp8_0t6iqz.partd\\\\.lock'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m ProgressBar():\n\u001b[0;32m     22\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(user_input) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m: \n\u001b[1;32m---> 23\u001b[0m         filtered_price\u001b[39m.\u001b[39mfrom_delayed \u001b[39m=\u001b[39m identify_state(user_input[\u001b[39m1\u001b[39;49m], filtered_price)\n\u001b[0;32m     26\u001b[0m \u001b[39mdel\u001b[39;00m user_input\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(filtered_price\u001b[39m.\u001b[39mcompute())\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36midentify_state\u001b[1;34m(user_input, df)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[39mreturn\u001b[39;00m chunk[chunk[\u001b[39m'\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(matched_state)]\n\u001b[0;32m     46\u001b[0m delayed_partitions \u001b[39m=\u001b[39m [delayed(process_chunk)(chunk) \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39mto_delayed()]\n\u001b[1;32m---> 48\u001b[0m results \u001b[39m=\u001b[39m compute(\u001b[39m*\u001b[39;49mdelayed_partitions, scheduler\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     50\u001b[0m \u001b[39mreturn\u001b[39;00m ddf\u001b[39m.\u001b[39mfrom_delayed(results, meta\u001b[39m=\u001b[39mdf)\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\threaded.py:89\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, keys, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     87\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 89\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     90\u001b[0m     pool\u001b[39m.\u001b[39;49msubmit,\n\u001b[0;32m     91\u001b[0m     pool\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[0;32m     92\u001b[0m     dsk,\n\u001b[0;32m     93\u001b[0m     keys,\n\u001b[0;32m     94\u001b[0m     cache\u001b[39m=\u001b[39;49mcache,\n\u001b[0;32m     95\u001b[0m     get_id\u001b[39m=\u001b[39;49m_thread_get_id,\n\u001b[0;32m     96\u001b[0m     pack_exception\u001b[39m=\u001b[39;49mpack_exception,\n\u001b[0;32m     97\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\local.py:511\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    510\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    512\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[0;32m    513\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\local.py:319\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 319\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[0;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\dask\\dataframe\\shuffle.py:951\u001b[0m, in \u001b[0;36mshuffle_group_3\u001b[1;34m(df, col, npartitions, p)\u001b[0m\n\u001b[0;32m    949\u001b[0m g \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mgroupby(col)\n\u001b[0;32m    950\u001b[0m d \u001b[39m=\u001b[39m {i: g\u001b[39m.\u001b[39mget_group(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m g\u001b[39m.\u001b[39mgroups}\n\u001b[1;32m--> 951\u001b[0m p\u001b[39m.\u001b[39;49mappend(d, fsync\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\encode.py:25\u001b[0m, in \u001b[0;36mEncode.append\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m data \u001b[39m=\u001b[39m valmap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode, data)\n\u001b[0;32m     24\u001b[0m data \u001b[39m=\u001b[39m valmap(frame, data)\n\u001b[1;32m---> 25\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartd\u001b[39m.\u001b[39;49mappend(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\buffer.py:45\u001b[0m, in \u001b[0;36mBuffer.append\u001b[1;34m(self, data, lock, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory_usage \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavailable_memory:\n\u001b[0;32m     44\u001b[0m         keys \u001b[39m=\u001b[39m keys_to_flush(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths, \u001b[39m0.1\u001b[39m, maxcount\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush(keys)\n\u001b[0;32m     47\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m lock: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\buffer.py:99\u001b[0m, in \u001b[0;36mBuffer.flush\u001b[1;34m(self, keys, block)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     97\u001b[0m     keys \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlengths)\n\u001b[1;32m---> 99\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslow\u001b[39m.\u001b[39;49mappend(\u001b[39mdict\u001b[39;49m(\u001b[39mzip\u001b[39;49m(keys, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfast\u001b[39m.\u001b[39;49mget(keys))))\n\u001b[0;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfast\u001b[39m.\u001b[39mdelete(keys)\n\u001b[0;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m keys:\n",
      "File \u001b[1;32mc:\\Users\\camel\\OneDrive - Arizona State University\\Desktop\\MyInflasi\\myenv\\Lib\\site-packages\\partd\\file.py:42\u001b[0m, in \u001b[0;36mFile.append\u001b[1;34m(self, data, lock, fsync, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(fn))\n\u001b[0;32m     41\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(fn, \u001b[39m'\u001b[39m\u001b[39mab\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> 42\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(v)\n\u001b[0;32m     43\u001b[0m     \u001b[39mif\u001b[39;00m fsync:\n\u001b[0;32m     44\u001b[0m         os\u001b[39m.\u001b[39mfsync(f)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "#dask.config.set({\"graphiz\" : \"C:\\Program Files\\Graphviz\\bin\\dot.exe\"})\n",
    "#dask.visualize()\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "if len(user_input) >= 1: \n",
    "    chunk_size = 100000\n",
    "\n",
    "    delayed_partitions = identify_item_code(user_input[0], lookup_dict, chunk_size)\n",
    "    results = compute(*delayed_partitions)\n",
    "    matching_item_codes = [item for sublist in results for item in sublist if item is not None]\n",
    "    del chunk_size\n",
    "\n",
    "filtered_price = price[price['item_code'].isin(matching_item_codes)]\n",
    "del matching_item_codes\n",
    "del delayed_partitions\n",
    "del lookup_dict\n",
    "del results\n",
    "\n",
    "with ProgressBar():\n",
    "    if len(user_input) >= 2: \n",
    "        filtered_price.from_delayed = identify_state(user_input[1], filtered_price)\n",
    "\n",
    "\n",
    "del user_input\n",
    "\n",
    "print(filtered_price.compute())\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
