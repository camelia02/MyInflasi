{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up and simplifying the dateframe to reduce memory and runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, do: pip install pandas fastparquet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import dask.dataframe as ddf\n",
    "from dask.diagnostics import ProgressBar\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_LOOKUP = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_item.parquet'\n",
    "URL_PREMISE = 'https://storage.googleapis.com/dosm-public-pricecatcher/lookup_premise.parquet'\n",
    "url_file = 'pricecatcher/pricecatcher/price_urls.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(url_file, 'r') as json_file: \n",
    "    price_urls_data = json.load(json_file)\n",
    "\n",
    "#\n",
    "#for entry in price_urls_data:\n",
    "#        parquet_urls = entry['parquet_files']\n",
    "#        for url in parquet_urls:\n",
    "#            df = ddf.read_parquet(url, blocksize = '1GB', npartitions = 20)\n",
    "#            if 'date' in df.columns: df['date'] = ddf.to_datetime(df['date'])\n",
    "#            df = df[(df['item_code'] != -1) | (df['premise_code'] != -1)]\n",
    "#            price_dfs.append(df)\n",
    "\n",
    "price_2022 = ddf.from_pandas(pd.DataFrame(), npartitions=12)\n",
    "price_2023 = ddf.from_pandas(pd.DataFrame(), npartitions=12)\n",
    "\n",
    "for entry in price_urls_data:\n",
    "    parquet_urls = entry['parquet_files']\n",
    "    for url in parquet_urls:\n",
    "        year = int(url.split('_')[1].split('-')[0])\n",
    "        df = ddf.read_parquet(url, blocksize = '1GB', npartitions = 12)\n",
    "        if 'date' in df.columns: df['date'] = ddf.to_datetime(df['date'])\n",
    "        df = df[(df['item_code'] != -1) | (df['premise_code'] != -1)]\n",
    "        df['premise_code'] = df['premise_code'].astype('int32')\n",
    "\n",
    "        if year == 2022: price_2022 = ddf.concat([price_2022, df], axis=0, ignore_index=True)\n",
    "        elif year == 2023: price_2023 = ddf.concat([price_2023, df], axis=0, ignore_index=True)\n",
    "\n",
    "premise = ddf.read_parquet(URL_PREMISE, npartitions = 8)\n",
    "lookup = ddf.read_parquet(URL_LOOKUP, npartitions = 8)\n",
    "premise = premise.dropna()\n",
    "lookup = lookup.dropna()\n",
    "if 'date' in lookup.columns: lookup['date'] = ddf.to_datetime(lookup['date'])\n",
    "if 'date' in premise.columns: premise['date'] = ddf.to_datetime(premise['date'])\n",
    "\n",
    "premise['premise_code'] = premise['premise_code'].astype('int32')\n",
    "premise = premise.drop(columns = ['premise_type', 'address', 'premise', 'district'])\n",
    "price_2022 = premise.merge(price_2022, on='premise_code', how='left', indicator=False)\n",
    "price_2023 = premise.merge(price_2023, on='premise_code', how='left', indicator=False)\n",
    "\n",
    "del df\n",
    "del json_file\n",
    "del parquet_urls\n",
    "del url\n",
    "del URL_PREMISE\n",
    "del URL_LOOKUP\n",
    "del url_file\n",
    "del price_urls_data\n",
    "del premise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A DICT SO USER CAN ACCESS USING ITEM AND PREMISE CODE INSTEAD OF NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(lookup.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {}\n",
    "\n",
    "for index, row in tqdm(lookup.iterrows(), total=len(lookup)):\n",
    "    key_tuple = tuple([row['item'], row['item_category']])\n",
    "    lookup_dict[key_tuple] = row['item_code']\n",
    "\n",
    "del key_tuple\n",
    "del row\n",
    "del index\n",
    "del lookup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DDF based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed, compute\n",
    "from distributed import Client, LocalCluster\n",
    "from fuzzywuzzy import fuzz\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key = []\n",
    "states = []\n",
    "\n",
    "def match(input, choices, threshold=80):\n",
    "        match_score = [(choice, fuzz.partial_ratio(input, choice.lower())) for choice in choices if isinstance(choice, str)]\n",
    "        matched_item = max(match_score, key=lambda x: x[1], default=None)\n",
    "        if matched_item[1] >= threshold:\n",
    "            return matched_item[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def match(input, choices, threshold=80):\n",
    "    match_score = [(choice, fuzz.partial_ratio(input, choice.lower())) for choice in choices if isinstance(choice, str)]\n",
    "    matched_item = max(match_score, key=lambda x: x[1], default=None)\n",
    "    if matched_item[1] >= threshold:\n",
    "        return matched_item[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_chunk(keys_chunk, user_item, lookup_dict):\n",
    "    matching_item_codes = []\n",
    "\n",
    "    for key in keys_chunk:\n",
    "        if match(user_item, [key[0], key[1]], threshold=80) is not None:\n",
    "            item_codes = lookup_dict[key]\n",
    "            matching_item_codes.append(item_codes)\n",
    "    return matching_item_codes\n",
    "\n",
    "def identify_item_code(user_input, lookup_dict, chunk_size=1000):\n",
    "    global key\n",
    "    user_item = user_input.lower()\n",
    "    keys = list(lookup_dict.keys())\n",
    "    key = keys\n",
    "    keys_chunks = [keys[i:i + chunk_size] for i in range(0, len(keys), chunk_size)]\n",
    "\n",
    "    delayed_partitions = []\n",
    "\n",
    "    for chunk in keys_chunks:\n",
    "        delayed_partitions.append(delayed(process_chunk)(chunk, user_item, lookup_dict))\n",
    "\n",
    "    return delayed_partitions\n",
    "\n",
    "def identify_state(user_input, df):\n",
    "    state_name = user_input.lower()\n",
    "    \n",
    "    def process_chunk(chunk):\n",
    "        matched_state = match(state_name, chunk['state'], threshold=80)\n",
    "        stats = matched_state\n",
    "        return chunk[chunk['state'].isin(matched_state)]\n",
    "    \n",
    "    delayed_partitions = [delayed(process_chunk)(chunk) for chunk in df.to_delayed()]\n",
    "\n",
    "    dask_results = [ddf.from_delayed(d, meta=df) for d in delayed_partitions]\n",
    "    final_result = ddf.concat(dask_results)\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just search using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = []\n",
    "item = 'ayam'\n",
    "state = 'johor'\n",
    "\n",
    "if (len(item.strip()) > 0) and (len(state.strip()) > 0): user_input = [item, state]\n",
    "elif (len(item.strip()) > 0): user_input = [item]\n",
    "del item\n",
    "del state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dask.config.set({\"graphiz\" : \"C:\\Program Files\\Graphviz\\bin\\dot.exe\"})\n",
    "#dask.visualize()\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "client = Client(dashboard_address=':8888')\n",
    "\n",
    "with ProgressBar():\n",
    "\n",
    "    if len(user_input) >= 1: \n",
    "        chunk_size = 10000\n",
    "\n",
    "        delayed_partitions = identify_item_code(user_input[0], lookup_dict, chunk_size)\n",
    "        results = compute(*delayed_partitions)\n",
    "        matching_item_codes = [item for sublist in results for item in sublist if item is not None]\n",
    "        del chunk_size\n",
    "\n",
    "    filtered_price = price_2022[price_2022['item_code'].isin(matching_item_codes)]\n",
    "    filtered_price = ddf.concat([filtered_price, price_2023[price_2023['item_code'].isin(matching_item_codes)]])\n",
    "    \n",
    "    del delayed_partitions\n",
    "    del lookup_dict\n",
    "    del results\n",
    "\n",
    "    if len(user_input) >= 2: \n",
    "        filtered_price.from_delayed = identify_state(user_input[1], filtered_price)\n",
    "\n",
    "#filtered_price_mapped = filtered_price.map(lambda df: df.compute(), meta=filtered_price)\n",
    "filtered_price_computed = filtered_price.persist()\n",
    "#result_df = filtered_price_computed.compute()\n",
    "\n",
    "client.close()\n",
    "cluster.close()\n",
    "\n",
    "del user_input\n",
    "del client\n",
    "del cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
